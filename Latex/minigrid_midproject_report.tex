\documentclass[11pt]{article}

% -------------------- Packages --------------------
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{siunitx}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

% -------------------- Title info (edit) --------------------
\title{\textbf{Mid Project: Tabular Reinforcement Learning in MiniGrid}\\
\large Monte Carlo, SARSA, and Q-Learning on Two Sparse-Reward Missions}
\author{
\textbf{Author 1 Name} (ID: \texttt{XXXXXXXXX}) \\
\textbf{Author 2 Name} (ID: \texttt{XXXXXXXXX}) \\
Course: RL2026A \qquad Semester: 2025 \\
}
\date{\today}

% -------------------- Convenience macros --------------------
\newcommand{\envEmpty}{\texttt{RandomEmptyEnv\_10}}
\newcommand{\envKey}{\texttt{RandomKeyMEnv\_10}}
\newcommand{\mc}{\textsc{First-Visit Monte Carlo}}
\newcommand{\sarsa}{\textsc{SARSA(0)}}
\newcommand{\qlearn}{\textsc{Q-Learning}}

\begin{document}
\maketitle

\begin{abstract}
This report studies tabular reinforcement learning in two MiniGrid tasks with sparse terminal rewards.
We compare \mc, \sarsa, and \qlearn under the same exploration scheme (epsilon-greedy with decay), and evaluate convergence, sample efficiency, and policy quality.
We report training curves (reward/success rate vs.\ episodes), and inference curves (average steps to solve).
Video rollouts during training and after convergence are included in the submitted notebook.
\end{abstract}

\section{Environments and MDP Analysis}
We evaluate two grid worlds (size $10\times 10$ with outer walls) implemented in the notebook:

\paragraph{Environment 1: \envEmpty.}
An empty room (no internal obstacles) with a randomized agent start position and direction.
The goal location is sampled from a small set of fixed coordinates.
The episode ends upon reaching the goal or when the step limit is reached.

\paragraph{Environment 2: \envKey.}
A two-room layout separated by a vertical wall with a locked door.
The key is placed on the left side; the agent must pick it up, open the door, and reach a goal on the right side.
The goal location is sampled from a small set of fixed coordinates.
The episode ends upon reaching the goal or step limit.

\subsection{Environment Type and Observability}
\textbf{Episodic:} Yes. Each run is a episodic task that terminates when the goal is reached (\texttt{terminated}) or when the maximum number of steps is exceeded (\texttt{truncated}).

\textbf{MDP:} Yes. MiniGrid is Markovian: the next state and reward depend only on the current environment state (agent position/direction, grid contents such as walls/door/key/goal, and relevant internal flags). 
With our \texttt{KeyFlatObsWrapper}, the agent observes a global encoding of the entire grid (plus the agent’s position and direction), so the task can be treated as an MDP from the agent’s point of view. 

\textbf{Action space:} Discrete. MiniGrid provides a small finite set of actions (typically 7): turn-left, turn-right, move-forward, pick-up, drop, toggle (e.g., open door), and done.

\textbf{State space:} Discrete and finite (but large). We learn a tabular $Q(s,a)$. In practice, we use a discretized state representation (a state tuple) rather than the full raw grid vector.

\textbf{Observability:} The environment becomes \emph{fully observable}. The wrapper exposes a global, flattened representation of the grid, and we additionally extract all relevant latent variables 
(agent position and direction, goal position, key position, door position, whether the agent is carrying the key, and whether the door is open).

\section{State Representation and Q-table Size}

\subsection{State definition used in the notebook}
Our tabular agents map each observation to a discrete state tuple extracted from the unwrapped MiniGrid environment:
\[
s = (x_a, y_a, d_a, x_g, y_g, x_k, y_k, x_d, y_d, \texttt{hasKey}, \texttt{doorOpen})
\]

Each component of the state tuple has the following meaning:
\begin{itemize}
  \item $x_a, y_a$ – the agent's grid coordinates.
  \item $d_a$ – the agent's orientation (direction), with
        $d_a \in \{0,1,2,3\}$ corresponding to the four cardinal directions.
  \item $x_g, y_g$ – the goal position coordinates.
  \item $x_k, y_k$ – the key position coordinates (or $(-1,-1)$ if no key exists or the key is not present).
  \item $x_d, y_d$ – the door position coordinates (or $(-1,-1)$ if no door exists).
  \item $\texttt{hasKey}$ – a binary flag indicating whether the agent is currently carrying the key.
  \item $\texttt{doorOpen}$ – a binary flag indicating whether the door is open.
\end{itemize}

This representation is fully observable and satisfies the Markov property.

\paragraph{Environment-dependent components.}
Not all state components contribute to the effective state-space complexity:
\begin{itemize}
  \item In \envEmpty\ (RandomEmptyEnv10), there is no key or door. The variables $(x_k,y_k)$ and $(x_d,y_d)$ are fixed sentinel values $(-1,-1)$, and $\texttt{hasKey}=\texttt{doorOpen}=0$.  
  These components are constant and therefore do \emph{not} increase the number of distinct states.
  
  \item In \envKey, all components are active and affect the transition dynamics.
\end{itemize}

\subsection{State space size estimation}

We consider a $10\times10$ MiniGrid with outer walls, so the agent occupies interior coordinates
$x,y \in \{1,\dots,8\}$, yielding $64$ positions, and $4$ orientations.

\paragraph{\envEmpty.}
The goal is sampled from $G=3$ possible positions. Since all other variables are constant:
\[
|\mathcal{S}_{\text{Empty}}|
=
64 \times 4 \times 3
=
768
\]

\paragraph{\envKey.}
In the key–door environment:
\begin{itemize}
  \item the goal has $G=2$ possible locations,
  \item the door is always located in column $3$ $\Rightarrow 8$ possible positions,
  \item the key is located on the left side of the wall (columns $1$ and $2$), giving $2 \times 8 = 16$ possible positions,
  \item $\texttt{hasKey} \in \{0,1\}$,
  \item $\texttt{doorOpen} \in \{0,1\}$.
\end{itemize}

The resulting theoretical state-space size is:
\[
|\mathcal{S}_{\text{Key}}|
=
64 \times 4 \times 2 \times 8 \times 16 \times 2 \times 2
=
262{,}144
\]

\subsection{Summary of theoretical state sizes}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Environment & Agent pos. & Orientation & Additional factors & $|\mathcal{S}|$ \\
\midrule
\envEmpty & $64$ & $4$ & $3$ goal positions & $768$ \\
\envKey   & $64$ & $4$ & $2 \cdot 8 \cdot 16 \cdot 2 \cdot 2$ & $262{,}144$ \\
\bottomrule
\end{tabular}
\caption{Theoretical number of discrete states under the full state representation used by the tabular agents.}
\end{table}

Let $|\mathcal{A}|$ be the number of discrete actions :  envEmpty $|\mathcal{A}|=3$, envKey $|\mathcal{A}|=7$.

\paragraph{Q-table size.}
The Q-table size is $|\mathcal{S}|\times|\mathcal{A}|$.  
In practice, Q-values are stored in a dictionary indexed by visited states, so memory usage scales with the number of states actually encountered during training rather than the full theoretical state space.
\section{Algorithms}
We compare three tabular reinforcement learning algorithms under the same
$\epsilon$-greedy exploration strategy.  
All methods operate on the discrete, fully observable state representation described in Section~X and are evaluated on two MiniGrid missions with sparse terminal rewards.

\subsection{\mc}
\textbf{Type:} On-policy, episodic Monte Carlo control.

Monte Carlo learning updates action values only after observing a complete episode.  
For each first visit of a state–action pair $(s,a)$, the return $G_t$ is computed and used to update:
\[
Q(s,a) \leftarrow Q(s,a) + \alpha\bigl(G_t - Q(s,a)\bigr).
\]

\textbf{Advantages in our MiniGrid missions:}
\begin{itemize}
  \item No bootstrapping bias: updates rely solely on actual observed returns.
  \item Conceptually simple and stable in strictly episodic environments.
\end{itemize}

\textbf{Disadvantages in our MiniGrid missions:}
\begin{itemize}
  \item Very high variance due to sparse terminal rewards.
  \item Credit assignment is difficult: all actions in a long episode receive the same delayed reward.
  \item Particularly slow in \envKey, where episodes are long and successful trajectories are rare early in training.
\end{itemize}

As a result, Monte Carlo learning converges slowly and requires many episodes to propagate useful information back to early states.

\subsection{\sarsa}
\textbf{Type:} On-policy temporal-difference (TD) control.

SARSA performs incremental updates at every step using the next action selected by the current behavior policy:
\[
Q(s,a)\leftarrow Q(s,a)+\alpha\Bigl(r+\gamma Q(s',a')-Q(s,a)\Bigr).
\]

\textbf{Advantages in our MiniGrid missions:}
\begin{itemize}
  \item Lower variance than Monte Carlo due to step-wise bootstrapping.
  \item Learns policies that account for exploration, leading to more conservative behavior.
  \item More stable learning than MC in sparse-reward settings.
\end{itemize}

\textbf{Disadvantages in our MiniGrid missions:}
\begin{itemize}
  \item Convergence to the optimal greedy policy can be slower than off-policy methods.
  \item Continued exploration (nonzero $\epsilon$) affects the learned value function.
\end{itemize}

In \envEmpty, SARSA learns efficiently due to the smaller state space.  
In \envKey, its conservative updates help stability but slow down optimal path learning.

\subsection{\qlearn}
\textbf{Type:} Off-policy temporal-difference (TD) control.

Q-learning updates toward the greedy value of the next state, independent of the behavior policy:
\[
Q(s,a)\leftarrow Q(s,a)+\alpha\Bigl(r+\gamma \max_{a'}Q(s',a')-Q(s,a)\Bigr).
\]

\textbf{Advantages in our MiniGrid missions:}
\begin{itemize}
  \item More sample-efficient than MC and SARSA.
  \item Faster propagation of terminal rewards through the state space.
  \item Performs well in large discrete state spaces such as \envKey.
\end{itemize}

\textbf{Disadvantages in our MiniGrid missions:}
\begin{itemize}
  \item Can overestimate action values due to the max operator.
  \item Sensitive to learning rate and exploration schedule.
\end{itemize}

Overall, Q-learning shows the fastest convergence in both environments, especially in the key–door task, where efficient reward propagation is crucial.

\subsection{Hyperparameters and initializations}
We test:
\begin{itemize}
  \item Learning rate $\alpha \in \{\texttt{...}\}$
  \item Epsilon decay $\epsilon_{decay} \in \{\texttt{...}\}$
  \item Discount $\gamma \in \{\texttt{...}\}$
  \item Initialization: zero initialization vs.\ optimistic initialization $Q_0>0$
\end{itemize}
(Replace the sets above with the exact grids used in the notebook.)

\section{Results}
This section must include:
\begin{itemize}
  \item \textbf{Convergence graphs:} reward (or success rate) vs.\ episodes for each algorithm.
  \item \textbf{Steps-to-solve:} number of steps to finish the episode for the most successful run, and average steps curves.
  \item \textbf{Inference evaluation:} greedy policy performance (average steps and success rate) measured over multiple episodes.
  \item \textbf{Videos:} middle of training and after convergence (included in notebook).
\end{itemize}

\subsection{Training curves (reward / success rate)}
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_reward_curve.png}
  \caption{\envEmpty: reward (or success rate) vs.\ episodes.}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_reward_curve.png}
  \caption{\envKey: reward (or success rate) vs.\ episodes.}
\end{subfigure}
\caption{Training convergence curves comparing \mc, \sarsa, and \qlearn.}
\label{fig:training_rewards}
\end{figure}

\subsection{Training curves (steps per episode)}
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_steps_curve.png}
  \caption{\envEmpty: steps/episode vs.\ episodes.}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_steps_curve.png}
  \caption{\envKey: steps/episode vs.\ episodes.}
\end{subfigure}
\caption{Training efficiency: average steps per episode during learning.}
\label{fig:training_steps}
\end{figure}

\subsection{Inference evaluation (greedy policy)}
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/infer_empty_avg_steps.png}
  \caption{\envEmpty: inference average steps.}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/infer_key_avg_steps.png}
  \caption{\envKey: inference average steps.}
\end{subfigure}
\caption{Inference stage: greedy policy evaluated over $N$ episodes (report $N$).}
\label{fig:inference_steps}
\end{figure}

\subsection{Best parameters cell (required)}
\textbf{In the notebook}, include a dedicated cell named \texttt{BEST\_PARAMS} that clearly reports:
\begin{itemize}
  \item Best hyperparameters per environment and algorithm: $\alpha$, $\gamma$, $\epsilon_0$, $\epsilon_{decay}$, $\epsilon_{min}$
  \item Best training run summary: episodes to reach consistent success, final success rate, and typical steps to solve
  \item Best inference summary: average steps and success rate for greedy policy
\end{itemize}
In this report, summarize the chosen best setting in Table~\ref{tab:best}.

\begin{table}[H]
\centering
\begin{tabular}{llcccc}
\toprule
Env & Algo & $\alpha$ & $\gamma$ & $\epsilon_{decay}$ & Inference avg steps \\
\midrule
\envEmpty & \mc     & \texttt{...} & \texttt{...} & \texttt{...} & \texttt{...} \\
\envEmpty & \sarsa  & \texttt{...} & \texttt{...} & \texttt{...} & \texttt{...} \\
\envEmpty & \qlearn & \texttt{...} & \texttt{...} & \texttt{...} & \texttt{...} \\
\addlinespace
\envKey   & \mc     & \texttt{...} & \texttt{...} & \texttt{...} & \texttt{...} \\
\envKey   & \sarsa  & \texttt{...} & \texttt{...} & \texttt{...} & \texttt{...} \\
\envKey   & \qlearn & \texttt{...} & \texttt{...} & \texttt{...} & \texttt{...} \\
\bottomrule
\end{tabular}
\caption{Best hyperparameters and inference performance (fill from notebook).}
\label{tab:best}
\end{table}

\section{Discussion}
\subsection{Advantages and disadvantages on our missions}
\paragraph{Sparse reward impact.}
Both tasks provide reward only at success, which makes exploration the main difficulty.
\mc often learns slower because it must complete successful trajectories to propagate credit information to earlier states.
TD methods (\sarsa, \qlearn) can propagate value estimates earlier via bootstrapping, typically improving sample efficiency.

\paragraph{On-policy vs off-policy.}
\sarsa learns values consistent with the behavior policy (epsilon-greedy), which can yield safer behavior during learning.
\qlearn learns a greedy target while exploring, often converging faster to an optimal greedy policy, but can be more sensitive to exploration settings.

\paragraph{Environment differences.}
\envEmpty is mostly navigation, so the reduced state $(x,y,dir,x_g,y_g)$ is close to Markov and learning is comparatively easy.
\envKey requires reasoning about \textit{key possession} and \textit{door state}; if these are excluded from the state representation, learning becomes harder because distinct situations collapse into the same tabular state.

\subsection{Exploration--Exploitation tradeoff}
We used epsilon-greedy with decay. Alternative or complementary approaches:
\begin{itemize}
  \item \textbf{Optimistic initialization:} start $Q(s,a)$ above 0 to encourage exploration early.
  \item \textbf{Exploring starts:} force random starting states/actions (when allowed) to improve coverage.
  \item \textbf{Different decay schedules:} linear decay, piecewise decay, or keeping a non-trivial $\epsilon_{min}$.
  \item \textbf{Count-based exploration (tabular):} add bonuses for rarely visited $(s,a)$.
\end{itemize}

\subsection{Strengths and limitations of our approach}
\paragraph{Good points.}
Tabular methods are simple, interpretable, and fast to run.
They are a strong baseline for small discrete tasks and allow clear comparisons between MC and TD control.

\paragraph{Bad points.}
State design is critical; an incomplete state (especially in \envKey) breaks the Markov property and can prevent full convergence.
Also, tabular approaches do not scale to larger grids or richer observations without function approximation.

\section{Conclusion}
Summarize which algorithm performed best in each environment and why (based on your plots and inference metrics), and propose improvements (better state, exploration bonuses, or function approximation).

\section*{Reproducibility Checklist}
\begin{itemize}
  \item Provide Colab notebook link: \texttt{<paste link>}
  \item Report random seeds used (if any): \texttt{<paste seeds>}
  \item Mention training episodes and evaluation episodes: \texttt{<numbers>}
  \item List best hyperparameters (Table~\ref{tab:best})
\end{itemize}

\end{document}
