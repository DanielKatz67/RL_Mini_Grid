\documentclass[11pt]{article}

% -------------------- Packages --------------------
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{siunitx}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

% -------------------- Title info (edit) --------------------
\title{\textbf{Mid Project: Tabular Reinforcement Learning in MiniGrid}\\
\large Monte Carlo, SARSA, and Q-Learning on Two Sparse-Reward Missions}
\author{
\textbf{Author 1: Avital Fine} (ID: \texttt{208253823}) \\
\textbf{Author 2: Daniel Katz} (ID: \texttt{315114991}) \\
Course: RL2026A \qquad Semester: 2025 \\
}
\date{\today}

% -------------------- Convenience macros --------------------
\newcommand{\envEmpty}{\texttt{RandomEmptyEnv\_10}}
\newcommand{\envKey}{\texttt{RandomKeyMEnv\_10}}
\newcommand{\mc}{\textsc{First-Visit Monte Carlo}}
\newcommand{\sarsa}{\textsc{SARSA(0)}}
\newcommand{\qlearn}{\textsc{Q-Learning}}

\begin{document}
\maketitle

\begin{abstract}
This report studies tabular reinforcement learning in two MiniGrid tasks with sparse terminal rewards.
We compare \mc, \sarsa, and \qlearn under the same exploration scheme (epsilon-greedy with decay), and evaluate convergence, sample efficiency, and policy quality.
We report training curves (reward/success rate vs.\ episodes), and inference curves (average steps to solve).
Video rollouts during training and after convergence are included in the submitted notebook.
\end{abstract}

\section{Environments and MDP Analysis}
We evaluate two grid worlds (size $10\times 10$ with outer walls) implemented in the notebook:

\paragraph{Environment 1: \envEmpty.}
This environment is an empty room, and the goal of the agent is to reach the green goal square.
Agent position at beginning is random.
Direction of the agent at beginning is random.
Goal position could be: $(8,1)$ or $(1,8)$ or $(8,8)$.
The episode ends upon reaching the goal or when the step limit is reached.
\textbf{Reward:} The agent receives $+1$ upon successfully reaching the goal (termination). No other rewards are defined.

\paragraph{Environment 2: \envKey.}
A two-room layout separated by a vertical wall with a locked door.
The key is placed on the left side; the agent must pick it up, open the door, and reach a goal on the right side.
The goal location is sampled from a small set of fixed coordinates.
The episode ends upon reaching the goal or step limit.
\textbf{Reward:} We use a shaped reward function: $+1$ upon reaching the goal, $+0.5$ for picking up the key (once), $+0.5$ for opening the door (once), and a step penalty of $-0.01$.

\subsection{Environment Type and Observability}
\textbf{Episodic:} Yes. Each run is a episodic task that terminates when the goal is reached (\texttt{terminated}) or when the maximum number of steps is exceeded (\texttt{truncated}).

\textbf{MDP:} Yes. MiniGrid is Markovian: the next state and reward depend only on the current environment state (agent position/direction, grid contents such as walls/door/key/goal, and relevant internal flags). 
With our \texttt{KeyFlatObsWrapper}, the agent observes a global encoding of the entire grid (plus the agent’s position and direction), so the task can be treated as an MDP from the agent’s point of view. 

\textbf{Action space:} Discrete. MiniGrid provides a small finite set of actions (typically 7): turn-left, turn-right, move-forward, pick-up, drop, toggle (e.g., open door), and done. In our specific implementation, we limit the action space by avoiding the following actions: \texttt{avoid\_actions=\{drop, done\}}.

\textbf{State space:} Discrete and finite (but large). We learn a tabular $Q(s,a)$. In practice, we use a discretized state representation (a state tuple) rather than the full raw grid vector.

\textbf{Observability:} The environment becomes \emph{fully observable}. The wrapper exposes a global, flattened representation of the grid, and we additionally extract all relevant latent variables 
(agent position and direction, goal position, key position, door position, whether the agent is carrying the key, and whether the door is open).

\section{State Representation and Q-table Size}

\subsection{State definition used in the notebook}
In the final notebook, the tabular state is taken directly from the environment observation returned by
\texttt{env.reset()} and \texttt{env.step(action)}.

We wrap MiniGrid with \texttt{KeyFlatObsWrapper}, which encodes the full grid state as follows:
the \(10\times10\) grid is encoded using MiniGrid’s 3-value cell representation
(object, color, state), the agent cell is overwritten to include the agent’s direction,
the outer walls are cropped, and the remaining \(8\times8\times3\) tensor is flattened
into a vector of length \(8\cdot8\cdot3=192\).
This flattened vector is stored as a tuple and used as the key in the Q-table.

\subsection{State space size estimation (reachable configurations)}
Below we give simple \emph{upper bounds} by multiplying together the main independent choices that can vary in our code.

\paragraph{\envEmpty\ (RandomEmptyEnv\_10).}
\begin{itemize}
  \item Agent position: interior is \(8\times 8\) \(\Rightarrow 64\) possibilities.
  \item Agent direction: \(4\) possibilities.
  \item Goal position: chosen from 3 fixed corners \(\Rightarrow 3\) possibilities.
\end{itemize}
So an upper bound is:
\[
|\mathcal{S}_{\text{empty}}| \le 64 \cdot 4 \cdot 3 = 768.
\]
With 3 actions (left/right/forward), the upper bound on Q-entries is \(768 \cdot 3 = 2304\).

\paragraph{\envKey\ (RandomKeyMEnv\_10).}
Here the observation can additionally change due to \emph{key location/carrying}, \emph{door location}, \emph{door state}, and \emph{goal location}.
Each number in the product below comes directly from our environment generation:

\begin{itemize}
  \item \textbf{Agent position:} interior \(8\times 8 \Rightarrow 64\).
  \item \textbf{Agent direction:} \(4\).
  \item \textbf{Key location:} in our code the key is placed in the left room with
        \(\texttt{x} \in \{1,2\}\) (2 columns) and \(\texttt{y} \in \{2,3,4,5,6,7,8\}\) (7 rows),
        so \(2 \cdot 7 = 14\) possible key tiles.
        Additionally, after pickup the key disappears from the grid and is \emph{carried} \(\Rightarrow +1\) extra case.
        Total: \(14 + 1 = 15\).
  \item \textbf{Door location:} the door row is sampled from \(\{1,\dots,8\}\) (any interior row),
        while the door column is fixed at the partition wall \(\Rightarrow 8\) possibilities.
  \item \textbf{Goal location:} chosen from \((8,1)\) or \((8,8)\) \(\Rightarrow 2\) possibilities.
  \item \textbf{Door state:} the door can be in up to 3 discrete states in MiniGrid encoding
        (locked / closed / open) \(\Rightarrow 3\) possibilities.
\end{itemize}

Putting it together:
\[
|\mathcal{S}_{\text{key}}|
\;\lesssim\;
(64)\cdot(4)\cdot(15)\cdot(8)\cdot(2)\cdot(3).
\]

So:
\[
|\mathcal{S}_{\text{key}}| \lesssim 184{,}320.
\]

In practice, we exclude two actions, \texttt{drop} and \texttt{done}, which are never useful in our setups.
Thus, the effective action set has \(|\mathcal{A}| = 5\) actions.
The resulting upper bound on Q-table entries is therefore:
\[
|\mathcal{S}_{\text{key}}|\cdot |\mathcal{A}|
\;\lesssim\;
184{,}320 \cdot 5
=
921{,}600.
\]

\paragraph{Important note (why this is only an upper bound).}
Not every combination above is actually reachable (e.g., when the agent stands on the key tile, the wrapper overwrites that cell with the agent encoding),
and our Q-table is stored \emph{sparsely} (a dictionary): we only create \((s,a)\) entries for states that are visited during training.

\section{Algorithms}
We compare three tabular reinforcement learning algorithms under the same
$\epsilon$-greedy exploration strategy.  
All methods operate on the discrete, fully observable state representation described in Section~X and are evaluated on two MiniGrid missions with sparse terminal rewards.

\subsection{\mc}
\textbf{Type:} On-policy, episodic Monte Carlo control.

Monte Carlo learning updates action values only after observing a complete episode.  
For each first visit of a state–action pair $(s,a)$, the return $G_t$ is computed and used to update:
\[
Q(s,a) \leftarrow Q(s,a) + \alpha\bigl(G_t - Q(s,a)\bigr).
\]

\subsection{\sarsa}
\textbf{Type:} On-policy temporal-difference (TD) control.

SARSA performs incremental updates at every step using the next action selected by the current behavior policy:
\[
Q(s,a)\leftarrow Q(s,a)+\alpha\Bigl(r+\gamma Q(s',a')-Q(s,a)\Bigr).
\]

\subsection{\qlearn}
\textbf{Type:} Off-policy temporal-difference (TD) control.

Q-learning updates toward the greedy value of the next state, independent of the behavior policy:
\[
Q(s,a)\leftarrow Q(s,a)+\alpha\Bigl(r+\gamma \max_{a'}Q(s',a')-Q(s,a)\Bigr).
\]

% =========================
% Hyperparameters
% =========================
\subsection{Hyperparameters and initializations}
We performed an extensive grid search to identify the optimal hyperparameters for each algorithm and environment. This process involved testing all combinations of the following parameters to ensure robust performance:

\begin{itemize}
  \item Learning rate: $\alpha \in \{0.1, 0.2\}$
  \item Discount factor: $\gamma \in \{0.9, 0.99\}$
  \item Epsilon decay: $\epsilon_{decay} \in \{0.999, 0.9995\}$
  \item Initial exploration: $\epsilon_0 = 1.0$, $\epsilon_{min} = 0.1$
  \item Initialization: zero initialization vs.\ optimistic initialization ($Q_0 > 0$)
\end{itemize}

After evaluating all combinations, we selected the parameters that yielded the best average reward and success rate. It is important to note that the accompanying notebook includes only the final, best-performing hyperparameter configurations. This was done to maintain clarity and focus on the most successful results, avoiding the clutter of hundreds of suboptimal experimental runs.

% =========================
% Results
% =========================
\section{Results}
We report results separately for each environment.  
All plots correspond to the best hyperparameter configuration found for each algorithm.

% -------------------------
% Empty Environment
% -------------------------
\subsection{\envEmpty\ (RandomEmptyEnv10)}

\subsubsection{Hyperparameters and Training}
Extensive grid search revealed the following optimal hyperparameters:
$\gamma=0.99$, $\alpha=0.2$, $\epsilon_0=1.0$, $\epsilon_{min}=0.1$, $\epsilon_{decay}=0.995$.

\textbf{Reward Structure:} The agent receives a reward of $+1$ upon termination (reaching the goal).

Figure~\ref{fig:empty_comparison} compares the training performance.
\begin{figure}[H]
\centering
\includegraphics[width=0.99\textwidth]{figs/emptyEnv_comparision.png}
\caption{\envEmpty: Training reward comparison. All algorithms successfully learn the task.}
\label{fig:empty_comparison}
\end{figure}

\subsubsection{Inference Evaluation}
The trained policies were evaluated over 100 episodes. The results are summarized below:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Algorithm & Avg Reward & Avg Steps & Success Rate \\
\midrule
Monte Carlo & $0.860$ & $36.8$ & 86.0\% \\
SARSA       & $1.000$ & $9.9$  & 100.0\% \\
Q-Learning  & $1.000$ & $9.2$  & 100.0\% \\
\bottomrule
\end{tabular}
\caption{Performance metrics on \envEmpty\ (100 evaluation episodes).}
\label{tab:empty_metrics}
\end{table}

Both SARSA and Q-Learning achieve 100\% success with near-optimal step counts. Monte Carlo is less stable with 86.0\% success.

\subsubsection{Agent Visualization}
We recorded the behavior of the trained Q-Learning agent:
\begin{figure}[H]
\centering
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_eval_1.png}
  \caption{Start}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_eval_2.png}
  \caption{Mid-trajectory}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_eval_3.png}
  \caption{Goal Reached}
\end{subfigure}
\caption{Visualizing the trained Q-Learning agent on \envEmpty. The agent efficiently navigates to the green goal square.}
\label{fig:empty_eval_vis}
\end{figure}

% -------------------------
% Key Environment
% -------------------------
\subsection{\envKey}

\subsubsection{Hyperparameters and Training}
For this environment, we used the following configuration:
$N_{episodes}=10000$, $\gamma=0.99$, $\alpha=0.1$, $\epsilon_0=1.0$, $\epsilon_{min}=0.1$, $\epsilon_{decay}=0.999$ (decay every 5 episodes).
The maximum steps per episode was set to 1000.

\textbf{Reward Structure:} The agent receives $+1$ upon termination (reaching the goal), $+0.5$ for picking up the key (can be done only once), $+0.5$ for opening the door (can be done only once), and a penalty of $-0.01$ for each step.

Figure~\ref{fig:key_comparison} compares the training performance.
\begin{figure}[H]
\centering
\includegraphics[width=0.99\textwidth]{figs/keyEnv_comparision.png}
\caption{\envKey: Training reward comparison.}
\label{fig:key_comparison}
\end{figure}

\subsubsection{Inference Evaluation}
The trained policies were evaluated over 100 episodes. The results are summarized below:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Algorithm & Avg Reward & Avg Steps & Success Rate \\
\midrule
Monte Carlo & $-2.342$ & $266.7$ & 12.0\% \\
SARSA       & $0.449$ & $99.1$  & 72.0\% \\
Q-Learning  & $1.496$ & $38.4$  & 94.0\% \\
\bottomrule
\end{tabular}
\caption{Performance metrics on \envKey\ (100 evaluation episodes).}
\label{tab:key_metrics}
\end{table}

\subsubsection{Agent Visualization}
We recorded the behavior of the trained agent:
\begin{figure}[H]
\centering
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_eval_1.png}
  \caption{Pick up Key}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_eval_2.png}
  \caption{Unlock Door}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_eval_3.png}
  \caption{Goal Reached}
\end{subfigure}
\caption{Visualizing the interactions in \envKey.}
\label{fig:key_eval_vis}
\end{figure}

% -------------------------
% Best Parameters Summary
% -------------------------
\subsection{Best hyperparameter summary}

\begin{table}[H]
\centering
\begin{tabular}{llcccc}
\toprule
Env & Algo & $\alpha$ & $\gamma$ & $\epsilon_{decay}$ & Inference avg steps \\
\midrule
\envEmpty & \mc     & 0.2 & 0.99 & 0.995  & 36.8 \\
\envEmpty & \sarsa  & 0.2 & 0.99 & 0.995  & 9.9 \\
\envEmpty & \qlearn & 0.2 & 0.99 & 0.995  & 9.2 \\
\addlinespace
\envKey   & \mc     & 0.1 & 0.99 & 0.999  & 266.7 \\
\envKey   & \sarsa  & 0.1 & 0.99 & 0.999  & 99.1 \\
\envKey   & \qlearn & 0.1 & 0.99 & 0.999  & 38.4 \\
\bottomrule
\end{tabular}
\caption{Best hyperparameters and inference performance per environment and algorithm.}
\label{tab:best}
\end{table}


\section{Discussion: Advantages and Disadvantages}

\subsection{Monte Carlo}
\textbf{Advantages:}
\begin{itemize}
    \item Simple and straightforward implementation
    \item Unbiased estimates (uses actual returns)
    \item Excellent for multi-step tasks: When one episode succeeds, ALL visited states learn from it
    \item Good baseline but struggled with \envKey\ (12\% success rate)
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item Must wait until episode ends to update
    \item High variance, especially for long episodes
    \item Cannot learn during an episode
\end{itemize}

\subsection{SARSA}
\textbf{Advantages:}
\begin{itemize}
    \item Online learning (updates every step)
    \item Lower variance than MC
    \item Learns conservative policies (good for risky environments)
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item Value propagates only one step per update
    \item Achieved moderate performance on \envKey\ (72\% success rate)
    \item May not find optimal policy without sufficient exploration
\end{itemize}

\subsection{Q-Learning}
\textbf{Advantages:}
\begin{itemize}
    \item Learns optimal policy regardless of behavior policy
    \item Off-policy learning allows more flexible exploration
    \item Best performer for \envKey\ (94\% success rate)
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
    \item Value propagates only one step per update
    \item Can overestimate Q-values (maximization bias)
\end{itemize}

\section{Key Findings for MiniGrid}
\textbf{Algorithm Selection Matters:} Q-Learning significantly outperforms Monte Carlo for the multi-step \envKey\ task. This demonstrates that off-policy TD methods can be more sample efficient.

\textbf{Value Propagation:} The critical difference is how value propagates:
\begin{itemize}
    \item MC: All states in a successful episode learn simultaneously
    \item TD: Value must propagate step-by-step over many episodes
\end{itemize}

\textbf{State Representation:} Fixed key/door positions (2,048 states) make the task tractable. Random positions would create $\sim$100k+ states.

\textbf{Reward Shaping:} Adding intermediate rewards ($+0.5$ for key, $+0.5$ for door) helps guide the agent, though it doesn't fully compensate for propagation delays.

\textbf{Exploration:} High epsilon (decaying to 0.1) is essential for discovering the goal path.

\section{Conclusion}
For episodic tasks with sparse rewards and long paths, Q-Learning proved to be the most robust algorithm in our experiments. While Monte Carlo is unbiased, it suffered from high variance and poor sample efficiency in the complex environment.
