\documentclass[11pt]{article}

% -------------------- Packages --------------------
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{siunitx}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

% -------------------- Title info (edit) --------------------
\title{\textbf{Mid Project: Tabular Reinforcement Learning in MiniGrid}\\
\large Monte Carlo, SARSA, and Q-Learning on Two Sparse-Reward Missions}
\author{
\textbf{Author 1: Avital Fine} (ID: \texttt{208253823}) \\
\textbf{Author 2: Daniel Katz} (ID: \texttt{315114991}) \\
Course: RL2026A \qquad Semester: 2025 \\
}
\date{\today}

% -------------------- Convenience macros --------------------
\newcommand{\envEmpty}{\texttt{RandomEmptyEnv\_10}}
\newcommand{\envKey}{\texttt{RandomKeyMEnv\_10}}
\newcommand{\mc}{\textsc{First-Visit Monte Carlo}}
\newcommand{\sarsa}{\textsc{SARSA(0)}}
\newcommand{\qlearn}{\textsc{Q-Learning}}

\begin{document}
\maketitle

\begin{abstract}
This report studies tabular reinforcement learning in two MiniGrid tasks with sparse terminal rewards.
We compare \mc, \sarsa, and \qlearn under the same exploration scheme (epsilon-greedy with decay), and evaluate convergence, sample efficiency, and policy quality.
We report training curves (reward/success rate vs.\ episodes), and inference curves (average steps to solve).
Video rollouts during training and after convergence are included in the submitted notebook.
\end{abstract}

\section{Environments and MDP Analysis}
We evaluate two grid worlds (size $10\times 10$ with outer walls) implemented in the notebook:

\paragraph{Environment 1: \envEmpty.}
This environment is an empty room, and the goal of the agent is to reach the green goal square.
Agent position at beginning is random.
Direction of the agent at beginning is random.
Goal position could be: $(8,1)$ or $(1,8)$ or $(8,8)$.
The episode ends upon reaching the goal or when the step limit is reached.
\textbf{Reward:} The agent receives $+1$ upon successfully reaching the goal (termination). No other rewards are defined.

\paragraph{Environment 2: \envKey.}
A two-room layout separated by a vertical wall with a locked door.
The key is placed on the left side; the agent must pick it up, open the door, and reach a goal on the right side.
The goal location is sampled from a small set of fixed coordinates.
The episode ends upon reaching the goal or step limit.
\textbf{Reward:} We use a shaped reward function: $+1$ upon reaching the goal, $+0.5$ for picking up the key (once), $+0.5$ for opening the door (once), and a step penalty of $-0.01$.

\subsection{Environment Type and Observability}
\textbf{Episodic:} Yes. Each run is a episodic task that terminates when the goal is reached (\texttt{terminated}) or when the maximum number of steps is exceeded (\texttt{truncated}).

\textbf{MDP:} Yes. MiniGrid is Markovian: the next state and reward depend only on the current environment state (agent position/direction, grid contents such as walls/door/key/goal, and relevant internal flags). 
With our \texttt{KeyFlatObsWrapper}, the agent observes a global encoding of the entire grid (plus the agent’s position and direction), so the task can be treated as an MDP from the agent’s point of view. 

\textbf{Action space:} Discrete. MiniGrid defines 7 actions:
turn-left, turn-right, move-forward, pick-up, drop, toggle, and done.
In our implementation we restrict actions only in \envKey\ by excluding
\texttt{avoid\_actions=\{drop, done\}}, leaving 5 effective actions.
For \envEmpty\ we keep the default action set (some actions are simply no-ops in this environment).

\textbf{State space:} Discrete and finite (but large). We learn a tabular $Q(s,a)$.
In practice, the state $s$ is the \emph{flattened observation vector} returned by
\texttt{env.reset()} / \texttt{env.step()}, after applying \texttt{KeyFlatObsWrapper}
(Section~\ref{sec:state_repr}).

\textbf{Observability:} Fully observable. With \texttt{KeyFlatObsWrapper}, the agent observes
a global encoding of the entire grid (including agent direction and door state in the grid encoding),
so we treat the problem as an MDP from the agent’s point of view (no additional latent variables are required).

\section{State Representation and Q-table Size}
\label{sec:state_repr}

\subsection{State definition used in the notebook}
In the final notebook, the tabular state is taken directly from the environment observation returned by
\texttt{env.reset()} and \texttt{env.step(action)}.

We wrap MiniGrid with \texttt{KeyFlatObsWrapper}, which encodes the full grid state as follows:
the \(10\times10\) grid is encoded using MiniGrid’s 3-value cell representation
(object, color, state), the agent cell is overwritten to include the agent’s direction,
the outer walls are cropped, and the remaining \(8\times8\times3\) tensor is flattened
into a vector of length \(8\cdot8\cdot3=192\).
This flattened vector is stored as a tuple and used as the key in the Q-table.

\subsection{State space size estimation (reachable configurations)}
Below we give simple \emph{upper bounds} by multiplying together the main independent choices that can vary in our code.

\paragraph{\envEmpty\ (RandomEmptyEnv\_10).}
\begin{itemize}
  \item Agent position: interior is \(8\times 8\) \(\Rightarrow 64\) possibilities.
  \item Agent direction: \(4\) possibilities.
  \item Goal position: chosen from 3 fixed corners \(\Rightarrow 3\) possibilities.
\end{itemize}
So an upper bound is:
\[
|\mathcal{S}_{\text{empty}}| \le 64 \cdot 4 \cdot 3 = 768.
\]
MiniGrid defines 7 actions. In \envEmpty\ only \{left, right, forward\} affect navigation,
while \{pick-up, drop, toggle, done\} are effectively no-ops; however, since we do not filter them
in this environment, the Q-table can still store values for all 7 actions.
Thus, an upper bound on Q-entries is \(768 \cdot 7 = 5376\).

\paragraph{\envKey\ (RandomKeyMEnv\_10).}
Here the observation can additionally change due to \emph{key location/carrying}, \emph{door location}, \emph{door state}, and \emph{goal location}.
Each number in the product below comes directly from our environment generation:

\begin{itemize}
  \item \textbf{Agent position:} interior \(8\times 8 \Rightarrow 64\).
  \item \textbf{Agent direction:} \(4\).
  \item \textbf{Key location:} in our code the key is placed in the left room with
        \(\texttt{x} \in \{1,2\}\) (2 columns) and \(\texttt{y} \in \{2,3,4,5,6,7,8\}\) (7 rows),
        so \(2 \cdot 7 = 14\) possible key tiles.
        Additionally, after pickup the key disappears from the grid and is \emph{carried} \(\Rightarrow +1\) extra case.
        Total: \(14 + 1 = 15\).
  \item \textbf{Door location:} the door row is sampled from \(\{1,\dots,8\}\) (any interior row),
        while the door column is fixed at the partition wall \(\Rightarrow 8\) possibilities.
  \item \textbf{Goal location:} chosen from \((8,1)\) or \((8,8)\) \(\Rightarrow 2\) possibilities.
  \item \textbf{Door state:} the door can be in up to 3 discrete states in MiniGrid encoding
        (locked / closed / open) \(\Rightarrow 3\) possibilities.
\end{itemize}

Putting it together:
\[
|\mathcal{S}_{\text{key}}|
\;\lesssim\;
(64)\cdot(4)\cdot(15)\cdot(8)\cdot(2)\cdot(3).
\]

So:
\[
|\mathcal{S}_{\text{key}}| \lesssim 184{,}320.
\]

In practice, we exclude two actions, \texttt{drop} and \texttt{done}, which are never useful in our setups.
Thus, the effective action set has \(|\mathcal{A}| = 5\) actions.
The resulting upper bound on Q-table entries is therefore:
\[
|\mathcal{S}_{\text{key}}|\cdot |\mathcal{A}|
\;\lesssim\;
184{,}320 \cdot 5
=
921{,}600.
\]

\paragraph{Important note (why this is only an upper bound).}
Not every combination above is actually reachable (e.g., when the agent stands on the key tile, the wrapper overwrites that cell with the agent encoding),
and our Q-table is stored \emph{sparsely} (a dictionary): we only create \((s,a)\) entries for states that are visited during training.

\section{Algorithms}
We compare three tabular reinforcement learning algorithms under the same
$\epsilon$-greedy exploration strategy.  
All methods operate on the discrete, fully observable state representation described in
Section~\ref{sec:state_repr} and are evaluated on two MiniGrid missions with sparse terminal rewards.

\subsection{\mc}
\textbf{Type:} On-policy, episodic Monte Carlo control.

Monte Carlo learning updates action values only after observing a complete episode.  
For each first visit of a state–action pair $(s,a)$, the return $G_t$ is computed and used to update:
\[
Q(s,a) \leftarrow Q(s,a) + \alpha\bigl(G_t - Q(s,a)\bigr).
\]

\subsection{\sarsa}
\textbf{Type:} On-policy temporal-difference (TD) control.

SARSA performs incremental updates at every step using the next action selected by the current behavior policy:
\[
Q(s,a)\leftarrow Q(s,a)+\alpha\Bigl(r+\gamma Q(s',a')-Q(s,a)\Bigr).
\]

\subsection{\qlearn}
\textbf{Type:} Off-policy temporal-difference (TD) control.

Q-learning updates toward the greedy value of the next state, independent of the behavior policy:
\[
Q(s,a)\leftarrow Q(s,a)+\alpha\Bigl(r+\gamma \max_{a'}Q(s',a')-Q(s,a)\Bigr).
\]

% =========================
% Hyperparameters
% =========================
\subsection{Hyperparameters and initializations}
We tuned $\alpha$, $\gamma$, and the $\epsilon$-greedy decay schedule separately for each environment and algorithm,
and we report only the best-performing configuration in the notebook outputs and figures.

\begin{itemize}
  \item Learning rate: $\alpha \in \{0.1, 0.2\}$
  \item Discount factor: $\gamma \in \{0.9, 0.99\}$
  \item Epsilon decay: $\epsilon_{decay} \in \{0.999, 0.9995\}$
  \item Initial exploration: $\epsilon_0 = 1.0$, $\epsilon_{min} = 0.1$
  \item Initialization: zero initialization vs.\ optimistic initialization ($Q_0 > 0$)
\end{itemize}

After evaluating all combinations, we selected the parameters that yielded the best average reward and success rate. 
It is important to note that the accompanying notebook includes only the final, best-performing hyperparameter configurations. 
This was done to maintain clarity and focus on the most successful results, avoiding the clutter of hundreds of suboptimal experimental runs.
The best hyperparameters per environment and algorithm are summarized in Table~\ref{tab:best}.

% =========================
% Results
% =========================
\section{Results}
We report results separately for each environment.  
All plots correspond to the best hyperparameter configuration found for each algorithm.

% -------------------------
% Empty Environment
% -------------------------
\subsection{\envEmpty\ (RandomEmptyEnv10)}

\subsubsection{Hyperparameters and Training}
Extensive grid search revealed the following optimal hyperparameters:
$\gamma=0.99$, $\alpha=0.2$, $\epsilon_0=1.0$, $\epsilon_{min}=0.1$, $\epsilon_{decay}=0.995$.

\textbf{Reward Structure:} The agent receives a reward of $+1$ upon termination (reaching the goal).

Figure~\ref{fig:empty_comparison} compares the training performance.
\begin{figure}[H]
\centering
\includegraphics[width=0.99\textwidth]{figs/emptyEnv_comparision.png}
\caption{\envEmpty: Training reward comparison. All algorithms successfully learn the task.}
\label{fig:empty_comparison}
\end{figure}

\subsubsection{Inference Evaluation}
The trained policies were evaluated over 100 episodes. The results are summarized below:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Algorithm & Avg Reward & Avg Steps & Success Rate \\
\midrule
Monte Carlo & $0.860$ & $36.8$ & 86.0\% \\
SARSA       & $1.000$ & $9.9$  & 100.0\% \\
Q-Learning  & $1.000$ & $9.2$  & 100.0\% \\
\bottomrule
\end{tabular}
\caption{Performance metrics on \envEmpty\ (100 evaluation episodes).}
\label{tab:empty_metrics}
\end{table}

Both SARSA and Q-Learning achieve 100\% success with near-optimal step counts. Monte Carlo is less stable with 86.0\% success.

\subsubsection{Agent Visualization}
We recorded the behavior of the trained Q-Learning agent:
\begin{figure}[H]
\centering
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_eval_1.png}
  \caption{Start}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_eval_2.png}
  \caption{Mid-trajectory}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/empty_eval_3.png}
  \caption{Goal Reached}
\end{subfigure}
\caption{Visualizing the trained Q-Learning agent on \envEmpty. The agent efficiently navigates to the green goal square.}
\label{fig:empty_eval_vis}
\end{figure}

% -------------------------
% Key Environment
% -------------------------
\subsection{\envKey}

\subsubsection{Hyperparameters and Training}
For this environment, we used the following configuration:
$N_{episodes}=10000$, $\gamma=0.99$, $\alpha=0.1$, $\epsilon_0=1.0$, $\epsilon_{min}=0.1$, $\epsilon_{decay}=0.999$ (decay every 5 episodes).
The maximum steps per episode was set to 1000.

\textbf{Reward Structure:} The agent receives $+1$ upon termination (reaching the goal), $+0.5$ for picking up the key (can be done only once), $+0.5$ for opening the door (can be done only once), and a penalty of $-0.01$ for each step.

Figure~\ref{fig:key_comparison} compares the training performance.
\begin{figure}[H]
\centering
\includegraphics[width=0.99\textwidth]{figs/keyEnv_comparision.png}
\caption{\envKey: Training reward comparison.}
\label{fig:key_comparison}
\end{figure}

\subsubsection{Inference Evaluation}
The trained policies were evaluated over 100 episodes. The results are summarized below:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Algorithm & Avg Reward & Avg Steps & Success Rate \\
\midrule
Monte Carlo & $-2.342$ & $266.7$ & 12.0\% \\
SARSA       & $0.449$ & $99.1$  & 72.0\% \\
Q-Learning  & $1.496$ & $38.4$  & 94.0\% \\
\bottomrule
\end{tabular}
\caption{Performance metrics on \envKey\ (100 evaluation episodes).}
\label{tab:key_metrics}
\end{table}

\subsubsection{Agent Visualization}
We recorded the behavior of the trained agent:
\begin{figure}[H]
\centering
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_eval_1.png}
  \caption{Pick up Key}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_eval_2.png}
  \caption{Unlock Door}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/key_eval_3.png}
  \caption{Goal Reached}
\end{subfigure}
\caption{Visualizing the interactions in \envKey.}
\label{fig:key_eval_vis}
\end{figure}

% -------------------------
% Best Parameters Summary
% -------------------------
\subsection{Best hyperparameter summary}

\begin{table}[H]
\centering
\begin{tabular}{llcccc}
\toprule
Env & Algo & $\alpha$ & $\gamma$ & $\epsilon_{decay}$ & Inference avg steps \\
\midrule
\envEmpty & \mc     & 0.2 & 0.99 & 0.995  & 36.8 \\
\envEmpty & \sarsa  & 0.2 & 0.99 & 0.995  & 9.9 \\
\envEmpty & \qlearn & 0.2 & 0.99 & 0.995  & 9.2 \\
\addlinespace
\envKey   & \mc     & 0.1 & 0.99 & 0.999  & 266.7 \\
\envKey   & \sarsa  & 0.1 & 0.99 & 0.999  & 99.1 \\
\envKey   & \qlearn & 0.1 & 0.99 & 0.999  & 38.4 \\
\bottomrule
\end{tabular}
\caption{Best hyperparameters and inference performance per environment and algorithm.}
\label{tab:best}
\end{table}


\section{Discussion: Advantages and Disadvantages}

We discuss the three algorithms in the context of our two missions and the observed results
(Empty: MC 86\%, SARSA/Q 100\%; Key: MC 12\%, SARSA 72\%, Q-Learning 94\%).

\subsection{Monte Carlo (\mc)}
\textbf{What worked well.}
In \envEmpty, \mc learns a reasonable policy (86\% success), since episodes are short and successful
trajectories are discovered relatively early, allowing returns to update many visited states at once.

\textbf{What failed / why.}
In \envKey, \mc performs poorly (12\% success, long episodes) because learning occurs only after an episode ends.
When success trajectories are rare, most episodes provide weak or misleading learning signals, and the variance of returns
is high (especially with long horizons and step penalty). Even with shaped rewards, the algorithm still needs many
successful full trajectories to stabilize value estimates.

\subsection{SARSA (\sarsa)}
\textbf{What worked well.}
SARSA improves substantially over MC on \envKey\ (72\% success). Because it bootstraps at every step,
it can learn incrementally even before consistently reaching the final goal.

\textbf{Tradeoff / limitation.}
SARSA is \emph{on-policy}: it updates using the next action actually taken under $\epsilon$-greedy exploration.
This tends to learn more conservative policies that account for exploratory mistakes, which can increase robustness,
but may slow convergence and lead to suboptimal paths compared to an off-policy greedy target.

\subsection{Q-Learning (\qlearn)}
\textbf{What worked well.}
Q-Learning achieved the best performance, especially on \envKey\ (94\% success, 38.4 average steps).
Being \emph{off-policy}, it updates toward $\max_{a'}Q(s',a')$, which accelerates value propagation toward the optimal
policy even while exploration remains high. This is particularly beneficial in multi-stage tasks
(key $\rightarrow$ door $\rightarrow$ goal) where credit assignment is difficult.

\textbf{Limitations.}
Q-Learning can suffer from maximization bias (overestimating action values due to the $\max$ operator),
though in our small deterministic domains this did not prevent strong performance.

\subsection{Environment-specific considerations and our approach}
\textbf{Reward shaping and step penalty.}
In \envKey\ we add intermediate rewards (+0.5 for picking the key once, +0.5 for opening the door once)
and a per-step penalty (-0.01). This improves exploration by making partial progress informative and encourages
shorter solutions during inference.

\textbf{Action filtering.}
We exclude \texttt{drop} and \texttt{done} in \envKey\ (5 effective actions). This reduces wasted exploration and
helps all methods concentrate on meaningful decisions.

\textbf{State representation.}
Using the flattened full-grid observation (192-length tuple) makes the task fully observable and easy to implement,
but it can increase the number of distinct visited states, slowing tabular learning.
A more compact semantic state (agent position/direction + key/door/goal factors) could further improve sample efficiency.

\section{Key Findings for MiniGrid}

\textbf{Algorithm choice matters most in multi-stage sparse tasks.}
On \envKey, Q-Learning (94\%) significantly outperformed SARSA (72\%) and Monte Carlo (12\%),
showing the advantage of off-policy TD updates with faster value propagation.

\textbf{Credit assignment is the core difficulty in \envKey.}
The agent must solve a sequence of subgoals (get key $\rightarrow$ open door $\rightarrow$ reach goal).
Monte Carlo struggles because successful full trajectories are rare; TD methods learn incrementally
from partial progress.

\textbf{Reward shaping helps exploration, but does not remove the need for efficient propagation.}
Intermediate rewards (+0.5 key, +0.5 door) and a step penalty (-0.01) make learning signals denser
and encourage shorter solutions, but Q-Learning still benefits most from its greedy target updates.

\textbf{Full-grid tabular states are simple but can be sample-inefficient.}
Our 192-length flattened grid encoding is fully observable, yet many distinct configurations may appear.
This motivates either stronger TD methods (as seen here) or a more compact state encoding for better generalization.

\textbf{Action filtering improves learning.}
Removing \texttt{drop} and \texttt{done} in \envKey\ reduces wasted exploration and improves convergence behavior.

\section{Conclusion}
This work compared three tabular reinforcement learning methods on two MiniGrid environments
with sparse terminal rewards.

In the simple \envEmpty\ task, all algorithms were able to learn effective policies, with
SARSA and Q-Learning achieving perfect success and near-optimal paths, while Monte Carlo
showed higher variance and slightly lower stability.

In the more challenging \envKey\ environment, which requires solving a sequence of subgoals,
Q-Learning proved to be the most robust and sample-efficient approach.
Its off-policy updates enabled faster value propagation across the long horizon,
leading to the highest success rate and shortest inference trajectories.
SARSA achieved moderate performance, while Monte Carlo struggled due to high variance and
the rarity of successful full episodes.

Overall, these results highlight the importance of algorithm choice, reward shaping,
and exploration strategy when applying tabular reinforcement learning to multi-step,
sparse-reward tasks with large state spaces.

\end{document}